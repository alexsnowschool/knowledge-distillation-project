# Knowledge distillation in Encoders

Knowledge distillation is a model compression technique in which a smaller and more efficient model is trained from a performant large/ensemble model.

