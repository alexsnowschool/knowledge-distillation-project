# Knowledge distillation in Encoders

Knowledge distillation is a model compression technique in which a smaller and more efficient model is trained from a performant large/ensemble model.

## Members
1. [Alex](https://github.com/alexsnow348)
2. [Stroeber](https://github.com/Stroeber)

## Paper and Article Resources
[Drive Folder](https://drive.google.com/drive/folders/1HIyQyMKAGG8HS33Sl6IZQJ8RiNRtOjle) 

1. Knowledge Distillation [Diego Slide](https://drive.google.com/file/d/1PK5TtwFsar-xdlI6fLivf3m_8PFRfuBQ/view?usp=sharing) - [MIT-Slide](https://drive.google.com/file/d/1knY6V0mWCgTZSSI-AdzboU80vejEGJru/view?usp=drive_link), [MIT - Video Lecture](https://youtu.be/EkjVHToId7U)
2. KD-related Papers - [Here](https://drive.google.com/drive/folders/1bso4pnfPDpKvuoTfXzIfzJ5eJgeiyTc5?usp=sharing)
   
## Source Code Resources
-- To Do
 
