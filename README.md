# Knowledge distillation in Encoders

Knowledge distillation is a model compression technique in which a smaller and more efficient model is trained from a performant large/ensemble model.

## Members
1. [Alex](https://github.com/alexsnow348)
2. [Stroeber](https://github.com/Stroeber)
3. [Brhaung](https://github.com/brhanug)
4. [Abin](https://github.com/abinbaby98)

## Paper and Article Resources
[Drive Folder](https://drive.google.com/drive/folders/1HIyQyMKAGG8HS33Sl6IZQJ8RiNRtOjle) 

1. Knowledge Distillation [Diego Slide](https://drive.google.com/file/d/1PK5TtwFsar-xdlI6fLivf3m_8PFRfuBQ/view?usp=sharing) - [MIT-Slide](https://drive.google.com/file/d/1knY6V0mWCgTZSSI-AdzboU80vejEGJru/view?usp=drive_link), [MIT - Video Lecture](https://youtu.be/EkjVHToId7U)
2. KD-related Papers - [Here](https://drive.google.com/drive/folders/1bso4pnfPDpKvuoTfXzIfzJ5eJgeiyTc5?usp=sharing)
3. [Transformer original paper (if you are not familiar with it, read it before Vision Transformer)](https://arxiv.org/abs/1706.03762)
6. [Vision Transformer (ViT) original paper](https://arxiv.org/pdf/2010.11929.pdf)
   
## Source Code Resources
1. [Knowledge distillation tutorial/introduction with Pytorch](https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)
2. [Knowledge Distillation theory heavy introduction (our case is feature-based knowledge and offline distillation)](https://neptune.ai/blog/knowledge-distillation)
3. [Github of baseline paper](https://github.com/DefangChen/SimKD/tree/main)
4. [Cluster tutorial/introduction](https://www.uni-hildesheim.de/gitlab/ismll/cluster-tutorial)

 



 
