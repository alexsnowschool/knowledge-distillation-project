# Knowledge distillation in Encoders

Knowledge distillation is a model compression technique in which a smaller and more efficient model is trained from a performant large/ensemble model.

## Members
1. [Alex](https://github.com/alexsnow348)
2. [Stroeber](https://github.com/Stroeber)

## Paper and Article Resources
1. [Drive Folder](https://drive.google.com/drive/folders/1HIyQyMKAGG8HS33Sl6IZQJ8RiNRtOjle)
   
## Source Code Resources
