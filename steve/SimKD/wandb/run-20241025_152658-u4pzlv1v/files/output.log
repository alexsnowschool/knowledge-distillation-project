Use GPU: 0 for training
==> loading teacher model
D:\Uni\student-research-project\steve\SimKD\train_student.py:300: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path, map_location=map_location)["model"])
==> done
6 1.0
Files already downloaded and verified
Files already downloaded and verified
Test: [0/313]	GPU: 0	Time: 11.320	Loss 0.7320	Acc@1 81.250	Acc@5 90.625
Test: [200/313]	GPU: 0	Time: 13.029	Loss 0.8817	Acc@1 79.260	Acc@5 94.527
teacher accuracy:  79.42
==> training...
Epoch: [1][0/782]	GPU 0	Time: 23.059	Loss 0.8634	Acc@1 0.000	Acc@5 6.250
Epoch: [1][200/782]	GPU 0	Time: 30.031	Loss 0.5772	Acc@1 1.617	Acc@5 7.261
Epoch: [1][400/782]	GPU 0	Time: 35.934	Loss 0.5363	Acc@1 1.707	Acc@5 7.789
Epoch: [1][600/782]	GPU 0	Time: 41.864	Loss 0.5214	Acc@1 1.744	Acc@5 7.979
 * Epoch 1, GPU 0, Acc@1 1.740, Acc@5 8.148, Time 72.23
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.080	Loss 4.5390	Acc@1 3.125	Acc@5 12.500
Test: [200/313]	GPU: 0	Time: 12.311	Loss 4.5611	Acc@1 1.975	Acc@5 8.147
 ** Acc@1 1.940, Acc@5 8.420
==> training...
Epoch: [2][0/782]	GPU 0	Time: 22.775	Loss 0.4832	Acc@1 1.562	Acc@5 14.062
Epoch: [2][200/782]	GPU 0	Time: 28.732	Loss 0.4826	Acc@1 1.905	Acc@5 8.644
Epoch: [2][400/782]	GPU 0	Time: 34.597	Loss 0.4824	Acc@1 1.948	Acc@5 8.818
Epoch: [2][600/782]	GPU 0	Time: 40.438	Loss 0.4820	Acc@1 1.973	Acc@5 8.730
 * Epoch 2, GPU 0, Acc@1 1.996, Acc@5 8.740, Time 69.17
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 9.715	Loss 4.5179	Acc@1 0.000	Acc@5 12.500
Test: [200/313]	GPU: 0	Time: 11.807	Loss 4.5539	Acc@1 1.741	Acc@5 8.053
 ** Acc@1 1.730, Acc@5 8.520
==> training...
Epoch: [3][0/782]	GPU 0	Time: 22.172	Loss 0.4980	Acc@1 0.000	Acc@5 10.938
Epoch: [3][200/782]	GPU 0	Time: 28.004	Loss 0.4782	Acc@1 2.122	Acc@5 8.598
Epoch: [3][400/782]	GPU 0	Time: 33.878	Loss 0.4775	Acc@1 2.120	Acc@5 8.853
Epoch: [3][600/782]	GPU 0	Time: 39.723	Loss 0.4771	Acc@1 2.030	Acc@5 8.803
 * Epoch 3, GPU 0, Acc@1 2.028, Acc@5 8.796, Time 68.73
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 9.764	Loss 4.5260	Acc@1 3.125	Acc@5 9.375
Test: [200/313]	GPU: 0	Time: 11.907	Loss 4.5420	Acc@1 2.223	Acc@5 8.380
 ** Acc@1 2.260, Acc@5 8.920
==> training...
Epoch: [4][0/782]	GPU 0	Time: 22.217	Loss 0.4573	Acc@1 0.000	Acc@5 3.125
Epoch: [4][200/782]	GPU 0	Time: 28.083	Loss 0.4719	Acc@1 2.363	Acc@5 9.056
Epoch: [4][400/782]	GPU 0	Time: 33.899	Loss 0.4712	Acc@1 2.209	Acc@5 9.028
Epoch: [4][600/782]	GPU 0	Time: 39.771	Loss 0.4708	Acc@1 2.119	Acc@5 8.785
 * Epoch 4, GPU 0, Acc@1 2.106, Acc@5 8.744, Time 68.59
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 9.866	Loss 4.5059	Acc@1 0.000	Acc@5 15.625
Test: [200/313]	GPU: 0	Time: 12.178	Loss 4.5372	Acc@1 1.912	Acc@5 8.178
 ** Acc@1 2.020, Acc@5 8.700
==> training...
Epoch: [5][0/782]	GPU 0	Time: 22.683	Loss 0.4712	Acc@1 0.000	Acc@5 4.688
Epoch: [5][200/782]	GPU 0	Time: 29.159	Loss 0.4686	Acc@1 2.037	Acc@5 9.375
Epoch: [5][400/782]	GPU 0	Time: 35.653	Loss 0.4676	Acc@1 2.396	Acc@5 10.236
Epoch: [5][600/782]	GPU 0	Time: 41.966	Loss 0.4668	Acc@1 2.649	Acc@5 11.028
 * Epoch 5, GPU 0, Acc@1 2.802, Acc@5 11.600, Time 73.01
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.206	Loss 4.3468	Acc@1 3.125	Acc@5 21.875
Test: [200/313]	GPU: 0	Time: 12.484	Loss 4.3840	Acc@1 3.762	Acc@5 14.506
 ** Acc@1 3.830, Acc@5 14.820
==> training...
