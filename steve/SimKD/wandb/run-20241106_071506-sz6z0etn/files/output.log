Use GPU: 0 for training
==> loading teacher model
D:\Uni\student-research-project\steve\SimKD\train_student.py:300: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_path, map_location=map_location)["model"])
==> done
6 1.0
Files already downloaded and verified
Files already downloaded and verified
Test: [0/313]	GPU: 0	Time: 10.960	Loss 0.7333	Acc@1 81.250	Acc@5 90.625
Test: [200/313]	GPU: 0	Time: 11.971	Loss 0.8817	Acc@1 79.260	Acc@5 94.527
teacher accuracy:  79.42
==> training...
Epoch: [1][0/782]	GPU 0	Time: 24.937	Loss 0.8277	Acc@1 0.000	Acc@5 7.812
Epoch: [1][200/782]	GPU 0	Time: 32.469	Loss 0.5920	Acc@1 1.197	Acc@5 5.683
Epoch: [1][400/782]	GPU 0	Time: 37.342	Loss 0.5496	Acc@1 1.368	Acc@5 6.051
Epoch: [1][600/782]	GPU 0	Time: 42.723	Loss 0.5331	Acc@1 1.438	Acc@5 6.338
 * Epoch 1, GPU 0, Acc@1 1.554, Acc@5 6.602, Time 71.11
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 11.337	Loss 4.6239	Acc@1 0.000	Acc@5 3.125
Test: [200/313]	GPU: 0	Time: 13.380	Loss 4.5944	Acc@1 1.819	Acc@5 6.483
 ** Acc@1 1.990, Acc@5 6.850
==> training...
Epoch: [2][0/782]	GPU 0	Time: 25.765	Loss 0.4950	Acc@1 0.000	Acc@5 1.562
Epoch: [2][200/782]	GPU 0	Time: 30.386	Loss 0.4970	Acc@1 1.702	Acc@5 7.237
Epoch: [2][400/782]	GPU 0	Time: 35.972	Loss 0.4963	Acc@1 1.898	Acc@5 7.649
Epoch: [2][600/782]	GPU 0	Time: 42.036	Loss 0.4951	Acc@1 1.976	Acc@5 7.880
 * Epoch 2, GPU 0, Acc@1 2.170, Acc@5 8.478, Time 70.30
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.418	Loss 4.5614	Acc@1 0.000	Acc@5 12.500
Test: [200/313]	GPU: 0	Time: 12.296	Loss 4.5208	Acc@1 2.581	Acc@5 11.132
 ** Acc@1 2.860, Acc@5 11.510
==> training...
Epoch: [3][0/782]	GPU 0	Time: 27.453	Loss 0.4553	Acc@1 0.000	Acc@5 9.375
Epoch: [3][200/782]	GPU 0	Time: 32.342	Loss 0.4913	Acc@1 2.853	Acc@5 10.798
Epoch: [3][400/782]	GPU 0	Time: 39.401	Loss 0.4901	Acc@1 2.833	Acc@5 11.117
Epoch: [3][600/782]	GPU 0	Time: 48.292	Loss 0.4894	Acc@1 2.839	Acc@5 11.483
 * Epoch 3, GPU 0, Acc@1 2.952, Acc@5 11.884, Time 97.89
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.471	Loss 4.3944	Acc@1 3.125	Acc@5 9.375
Test: [200/313]	GPU: 0	Time: 12.316	Loss 4.4288	Acc@1 3.032	Acc@5 13.324
 ** Acc@1 3.280, Acc@5 13.810
==> training...
Epoch: [4][0/782]	GPU 0	Time: 27.434	Loss 0.4828	Acc@1 3.125	Acc@5 17.188
Epoch: [4][200/782]	GPU 0	Time: 30.983	Loss 0.4832	Acc@1 3.249	Acc@5 13.666
Epoch: [4][400/782]	GPU 0	Time: 33.704	Loss 0.4823	Acc@1 3.472	Acc@5 14.281
Epoch: [4][600/782]	GPU 0	Time: 36.910	Loss 0.4824	Acc@1 3.557	Acc@5 14.567
 * Epoch 4, GPU 0, Acc@1 3.662, Acc@5 14.910, Time 61.66
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.285	Loss 4.3292	Acc@1 3.125	Acc@5 12.500
Test: [200/313]	GPU: 0	Time: 12.654	Loss 4.3376	Acc@1 3.809	Acc@5 16.807
 ** Acc@1 3.990, Acc@5 16.960
==> training...
Epoch: [5][0/782]	GPU 0	Time: 23.799	Loss 0.4791	Acc@1 3.125	Acc@5 12.500
Epoch: [5][200/782]	GPU 0	Time: 29.176	Loss 0.4783	Acc@1 4.322	Acc@5 17.102
Epoch: [5][400/782]	GPU 0	Time: 32.195	Loss 0.4776	Acc@1 4.228	Acc@5 17.254
Epoch: [5][600/782]	GPU 0	Time: 35.046	Loss 0.4775	Acc@1 4.303	Acc@5 17.434
 * Epoch 5, GPU 0, Acc@1 4.352, Acc@5 17.832, Time 50.16
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 12.931	Loss 4.2614	Acc@1 6.250	Acc@5 9.375
Test: [200/313]	GPU: 0	Time: 14.996	Loss 4.2453	Acc@1 5.006	Acc@5 19.139
 ** Acc@1 5.020, Acc@5 19.560
==> training...
Epoch: [6][0/782]	GPU 0	Time: 22.958	Loss 0.4855	Acc@1 1.562	Acc@5 17.188
Epoch: [6][200/782]	GPU 0	Time: 30.127	Loss 0.4745	Acc@1 4.649	Acc@5 19.022
Epoch: [6][400/782]	GPU 0	Time: 36.574	Loss 0.4734	Acc@1 4.758	Acc@5 19.459
Epoch: [6][600/782]	GPU 0	Time: 41.873	Loss 0.4731	Acc@1 4.791	Acc@5 19.366
 * Epoch 6, GPU 0, Acc@1 4.774, Acc@5 19.540, Time 61.61
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.350	Loss 4.1666	Acc@1 3.125	Acc@5 18.750
Test: [200/313]	GPU: 0	Time: 11.739	Loss 4.1741	Acc@1 5.162	Acc@5 20.569
 ** Acc@1 5.330, Acc@5 20.830
==> training...
Epoch: [7][0/782]	GPU 0	Time: 23.631	Loss 0.4806	Acc@1 4.688	Acc@5 20.312
Epoch: [7][200/782]	GPU 0	Time: 30.357	Loss 0.4715	Acc@1 5.131	Acc@5 20.452
Epoch: [7][400/782]	GPU 0	Time: 37.616	Loss 0.4704	Acc@1 5.221	Acc@5 20.815
Epoch: [7][600/782]	GPU 0	Time: 43.392	Loss 0.4696	Acc@1 5.306	Acc@5 21.064
 * Epoch 7, GPU 0, Acc@1 5.338, Acc@5 21.290, Time 71.91
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.464	Loss 4.1322	Acc@1 9.375	Acc@5 18.750
Test: [200/313]	GPU: 0	Time: 11.900	Loss 4.1065	Acc@1 6.110	Acc@5 22.870
 ** Acc@1 6.200, Acc@5 23.130
==> training...
Epoch: [8][0/782]	GPU 0	Time: 24.482	Loss 0.4495	Acc@1 0.000	Acc@5 10.938
Epoch: [8][200/782]	GPU 0	Time: 31.165	Loss 0.4655	Acc@1 5.955	Acc@5 22.676
Epoch: [8][400/782]	GPU 0	Time: 37.386	Loss 0.4640	Acc@1 6.020	Acc@5 22.993
Epoch: [8][600/782]	GPU 0	Time: 43.630	Loss 0.4644	Acc@1 6.104	Acc@5 23.248
 * Epoch 8, GPU 0, Acc@1 6.268, Acc@5 23.526, Time 73.66
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.591	Loss 3.9998	Acc@1 6.250	Acc@5 21.875
Test: [200/313]	GPU: 0	Time: 11.920	Loss 4.0091	Acc@1 6.950	Acc@5 25.731
 ** Acc@1 7.150, Acc@5 26.000
==> training...
Epoch: [9][0/782]	GPU 0	Time: 25.264	Loss 0.4428	Acc@1 4.688	Acc@5 25.000
Epoch: [9][200/782]	GPU 0	Time: 30.254	Loss 0.4629	Acc@1 6.491	Acc@5 24.324
Epoch: [9][400/782]	GPU 0	Time: 36.791	Loss 0.4617	Acc@1 6.807	Acc@5 25.000
Epoch: [9][600/782]	GPU 0	Time: 43.623	Loss 0.4604	Acc@1 7.046	Acc@5 25.354
 * Epoch 9, GPU 0, Acc@1 7.086, Acc@5 25.538, Time 78.72
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.393	Loss 3.9249	Acc@1 6.250	Acc@5 25.000
Test: [200/313]	GPU: 0	Time: 12.096	Loss 3.9696	Acc@1 7.758	Acc@5 26.990
 ** Acc@1 7.780, Acc@5 27.220
==> training...
Epoch: [10][0/782]	GPU 0	Time: 24.045	Loss 0.4590	Acc@1 12.500	Acc@5 28.125
Epoch: [10][200/782]	GPU 0	Time: 28.628	Loss 0.4564	Acc@1 7.603	Acc@5 26.726
Epoch: [10][400/782]	GPU 0	Time: 34.032	Loss 0.4566	Acc@1 7.945	Acc@5 27.248
Epoch: [10][600/782]	GPU 0	Time: 40.374	Loss 0.4570	Acc@1 8.117	Acc@5 27.355
 * Epoch 10, GPU 0, Acc@1 8.212, Acc@5 27.668, Time 74.40
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.576	Loss 3.8188	Acc@1 15.625	Acc@5 25.000
Test: [200/313]	GPU: 0	Time: 12.008	Loss 3.9158	Acc@1 8.815	Acc@5 28.933
 ** Acc@1 8.890, Acc@5 29.520
==> training...
Epoch: [11][0/782]	GPU 0	Time: 24.366	Loss 0.4383	Acc@1 9.375	Acc@5 34.375
Epoch: [11][200/782]	GPU 0	Time: 28.283	Loss 0.4533	Acc@1 8.427	Acc@5 28.825
Epoch: [11][400/782]	GPU 0	Time: 31.299	Loss 0.4538	Acc@1 8.471	Acc@5 29.025
Epoch: [11][600/782]	GPU 0	Time: 35.906	Loss 0.4535	Acc@1 8.754	Acc@5 29.490
 * Epoch 11, GPU 0, Acc@1 8.848, Acc@5 29.684, Time 65.14
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.413	Loss 3.8108	Acc@1 6.250	Acc@5 25.000
Test: [200/313]	GPU: 0	Time: 12.878	Loss 3.8522	Acc@1 9.670	Acc@5 31.950
 ** Acc@1 9.720, Acc@5 32.160
==> training...
Epoch: [12][0/782]	GPU 0	Time: 24.703	Loss 0.4476	Acc@1 9.375	Acc@5 31.250
Epoch: [12][200/782]	GPU 0	Time: 27.456	Loss 0.4482	Acc@1 9.608	Acc@5 31.475
Epoch: [12][400/782]	GPU 0	Time: 30.230	Loss 0.4487	Acc@1 9.453	Acc@5 31.421
Epoch: [12][600/782]	GPU 0	Time: 33.341	Loss 0.4495	Acc@1 9.755	Acc@5 31.788
 * Epoch 12, GPU 0, Acc@1 9.800, Acc@5 31.898, Time 53.43
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.567	Loss 3.6949	Acc@1 12.500	Acc@5 31.250
Test: [200/313]	GPU: 0	Time: 13.084	Loss 3.7843	Acc@1 11.334	Acc@5 33.955
 ** Acc@1 11.210, Acc@5 34.280
==> training...
Epoch: [13][0/782]	GPU 0	Time: 24.027	Loss 0.4570	Acc@1 9.375	Acc@5 34.375
Epoch: [13][200/782]	GPU 0	Time: 29.331	Loss 0.4469	Acc@1 10.588	Acc@5 33.481
Epoch: [13][400/782]	GPU 0	Time: 34.289	Loss 0.4471	Acc@1 10.747	Acc@5 33.884
Epoch: [13][600/782]	GPU 0	Time: 37.788	Loss 0.4464	Acc@1 10.847	Acc@5 34.076
 * Epoch 13, GPU 0, Acc@1 10.868, Acc@5 34.166, Time 54.39
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.563	Loss 3.6588	Acc@1 18.750	Acc@5 31.250
Test: [200/313]	GPU: 0	Time: 12.543	Loss 3.7275	Acc@1 11.816	Acc@5 36.287
 ** Acc@1 12.190, Acc@5 36.810
==> training...
Epoch: [14][0/782]	GPU 0	Time: 23.596	Loss 0.4257	Acc@1 17.188	Acc@5 42.188
Epoch: [14][200/782]	GPU 0	Time: 32.099	Loss 0.4422	Acc@1 11.925	Acc@5 36.303
Epoch: [14][400/782]	GPU 0	Time: 38.869	Loss 0.4422	Acc@1 12.305	Acc@5 36.530
Epoch: [14][600/782]	GPU 0	Time: 44.668	Loss 0.4422	Acc@1 12.336	Acc@5 36.868
 * Epoch 14, GPU 0, Acc@1 12.336, Acc@5 36.902, Time 66.17
GPU 0 validating
Test: [0/313]	GPU: 0	Time: 10.383	Loss 3.5625	Acc@1 18.750	Acc@5 37.500
Test: [200/313]	GPU: 0	Time: 11.820	Loss 3.6691	Acc@1 13.511	Acc@5 38.977
 ** Acc@1 13.520, Acc@5 38.940
==> training...
Epoch: [15][0/782]	GPU 0	Time: 23.780	Loss 0.4519	Acc@1 9.375	Acc@5 35.938
Epoch: [15][200/782]	GPU 0	Time: 32.318	Loss 0.4401	Acc@1 12.826	Acc@5 38.231
Epoch: [15][400/782]	GPU 0	Time: 39.781	Loss 0.4403	Acc@1 12.971	Acc@5 38.525
Epoch: [15][600/782]	GPU 0	Time: 46.337	Loss 0.4401	Acc@1 13.095	Acc@5 38.792
Traceback (most recent call last):
  File "D:\programming\miniconda3\envs\SRP_steve\Lib\site-packages\torch\utils\data\dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\programming\miniconda3\envs\SRP_steve\Lib\multiprocessing\queues.py", line 122, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\programming\miniconda3\envs\SRP_steve\Lib\site-packages\torch\multiprocessing\reductions.py", line 515, in rebuild_storage_filename
    storage = torch.UntypedStorage._new_shared_filename_cpu(manager, handle, size)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Couldn't open shared event: <torch_25368_77479101_186_event>, error code: <2>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Uni\student-research-project\steve\SimKD\train_student.py", line 683, in <module>
    main()
  File "D:\Uni\student-research-project\steve\SimKD\train_student.py", line 327, in main
    main_worker(None if ngpus_per_node > 1 else opt.gpu_id, ngpus_per_node, opt)
  File "D:\Uni\student-research-project\steve\SimKD\train_student.py", line 579, in main_worker
    train_acc, train_acc_top5, train_loss = train(
                                            ^^^^^^
  File "D:\Uni\student-research-project\steve\SimKD\helper\loops.py", line 93, in train_distill
    for idx, data in enumerate(train_loader):
  File "D:\programming\miniconda3\envs\SRP_steve\Lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "D:\programming\miniconda3\envs\SRP_steve\Lib\site-packages\torch\utils\data\dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
                ^^^^^^^^^^^^^^^^
  File "D:\programming\miniconda3\envs\SRP_steve\Lib\site-packages\torch\utils\data\dataloader.py", line 1293, in _get_data
    success, data = self._try_get_data()
                    ^^^^^^^^^^^^^^^^^^^^
  File "D:\programming\miniconda3\envs\SRP_steve\Lib\site-packages\torch\utils\data\dataloader.py", line 1144, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 25368) exited unexpectedly
