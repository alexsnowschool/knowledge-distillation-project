# Knowledge distillation in Encoders

Knowledge distillation is a model compression technique in which a smaller and more efficient model is trained from a performant large/ensemble model.

## Members
1. [Alex](https://github.com/alexsnow348) - Wut Hmone Hnin Hlaing - 1748517
2. [Stroeber](https://github.com/Stroeber) - Steffen Roeber - 1748613
3. [Apurv](https://github.com/Apurv505) - Apurv Kumar  - 1748394
4. [Abin](https://github.com/abinbaby98) - Abin Baby - 1748437
5. [Brhaung](https://github.com/brhanug) - Brhanu Atsbaha - 1748398

1. Knowledge Distillation [Diego Slide](https://drive.google.com/file/d/1PK5TtwFsar-xdlI6fLivf3m_8PFRfuBQ/view?usp=sharing) - [MIT-Slide](https://drive.google.com/file/d/1knY6V0mWCgTZSSI-AdzboU80vejEGJru/view?usp=drive_link), [MIT - Video Lecture](https://youtu.be/EkjVHToId7U)
2. KD-related Papers - [Here](https://drive.google.com/drive/folders/1bso4pnfPDpKvuoTfXzIfzJ5eJgeiyTc5?usp=sharing)
3. [Transformer original paper (if you are not familiar with it, read it before Vision Transformer)](https://arxiv.org/abs/1706.03762)
6. [Vision Transformer (ViT) original paper](https://arxiv.org/pdf/2010.11929.pdf)
   
## Source Code Resources
1. [Knowledge distillation tutorial/introduction with Pytorch](https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)
2. [Knowledge Distillation theory heavy introduction (our case is feature-based knowledge and offline distillation)](https://neptune.ai/blog/knowledge-distillation)
3. [Github of baseline paper](https://github.com/DefangChen/SimKD/tree/main)
4. [Cluster tutorial/introduction](https://www.uni-hildesheim.de/gitlab/ismll/cluster-tutorial)

 



 
